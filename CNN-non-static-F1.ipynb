{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# These are all the modules we'll be using later. Make sure you can import them\n",
      "# before proceeding further.\n",
      "from __future__ import print_function\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
        "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def readSentence2Array(sentenceFileName,inputMaxLength = 128 , inputMaxLengthFlag = 1):\n",
      "    file = open(sentenceFileName,'r')\n",
      "    lines = file.readlines()\n",
      "    sentenceList = []\n",
      "    for line in lines:\n",
      "        splitList = line.strip().split(' ')\n",
      "        sentenceList.append(splitList)\n",
      "    maxLength = len(sentenceList[0])\n",
      "    for row in sentenceList:\n",
      "        if maxLength <= len(row):\n",
      "            maxLength = len(row)\n",
      "    if inputMaxLengthFlag == 0: \n",
      "        for row in sentenceList:\n",
      "            for i in range(len(row),maxLength):\n",
      "                row.append('0')\n",
      "                \n",
      "    elif inputMaxLengthFlag == 1:\n",
      "        temp_list = []\n",
      "        for row in sentenceList:\n",
      "            if len(row) < inputMaxLength:\n",
      "                for i in range(len(row),inputMaxLength):\n",
      "                    row.append('0')\n",
      "                temp_list.append(row)\n",
      "            elif len(row) >= inputMaxLength:\n",
      "                temp_list.append(row[0:inputMaxLength])\n",
      "        sentenceList = temp_list\n",
      "        \n",
      "    numSentenceList = strList2numList2D(sentenceList)\n",
      "    print (np.array(numSentenceList))\n",
      "    return np.array(numSentenceList)\n",
      "\n",
      "def strList2numList2D(strlist):\n",
      "    copyList = []\n",
      "    for sentence in strlist:\n",
      "        temp_list = []\n",
      "        for number in sentence:\n",
      "            temp_list.append(int(number))\n",
      "        copyList.append(temp_list)\n",
      "    return copyList\n",
      "\n",
      "def readLabelFile2Array(LabelFileName):\n",
      "    file = open(LabelFileName,'r')\n",
      "    lines = file.readlines()\n",
      "    labelList = []\n",
      "    for line in lines:\n",
      "        splitList = line.strip()\n",
      "        labelList.append(splitList)\n",
      "    return np.array(labelList)\n",
      "\n",
      "\n",
      "savedArrayFile = r'/media/huxi/\u65b0\u52a0\u53771/NLP_exLab/pythonCode/CNN/data/hotel_annotatedData_utf8/non-static/embeddingVocabulary.npy'\n",
      "wordEmbedding = np.load(savedArrayFile)\n",
      "\n",
      "lengthEmbedding = np.shape(wordEmbedding)[1]#\u8bcd\u5411\u91cf\u957f\u5ea6\n",
      "lengthVocabulary = np.shape(wordEmbedding)[0]#\u8bcd\u5178\u957f\u5ea6\n",
      "\n",
      "sentenceFileName = r'/media/huxi/\u65b0\u52a0\u53771/NLP_exLab/pythonCode/CNN/data/hotel_annotatedData_utf8/non-static/sentence_index.txt'\n",
      "sentenceArray = readSentence2Array(sentenceFileName)\n",
      "num_array = np.shape(sentenceArray)[0]#\u53e5\u5b50\u6761\u6570\n",
      "sentenceLength = np.shape(sentenceArray)[1]#\u53e5\u5b50\u957f\u5ea6\n",
      "print (sentenceLength)\n",
      "\n",
      "train_dataset = sentenceArray[0:int(num_array*0.8)]\n",
      "#train_labels = save['train_labels']\n",
      "valid_dataset = sentenceArray[int(num_array*0.8):int(num_array*0.8)+int(num_array*0.1)]\n",
      "#valid_labels = save['valid_labels']\n",
      "test_dataset = sentenceArray[int(num_array*0.8)+int(num_array*0.1):]\n",
      "#test_labels = save['test_labels']\n",
      "\n",
      "LabelFileName =  r'/media/huxi/\u65b0\u52a0\u53771/NLP_exLab/pythonCode/CNN/data/hotel_annotatedData_utf8/shuffData_result/shuffledLabel.txt'\n",
      "labels = readLabelFile2Array(LabelFileName)\n",
      "\n",
      "train_labels = labels[0:int(num_array*0.8)]\n",
      "valid_labels = labels[int(num_array*0.8):int(num_array*0.8)+int(num_array*0.1)]\n",
      "test_labels = labels[int(num_array*0.8)+int(num_array*0.1):]\n",
      "#del save  # hint to help gc free up memory\n",
      "#print (train_labels)\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 9263  9508  2557 ..., 11645 12949  1690]\n",
        " [ 9946   306 11544 ...,     0     0     0]\n",
        " [ 4274  5417  8530 ...,     0     0     0]\n",
        " ..., \n",
        " [ 4274  8764  5013 ...,     0     0     0]\n",
        " [ 5417 10027  6540 ...,     0     0     0]\n",
        " [ 3720  3955 10958 ...,     0     0     0]]\n",
        "128\n",
        "Training set (3804, 128) (3804,)\n",
        "Validation set (475, 128) (475,)\n",
        "Test set (476, 128) (476,)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_labels = 2\n",
      "num_channels = 1\n",
      "\n",
      "\n",
      "def reformat(dataset,labels):\n",
      "  label_T  = []\n",
      "  #print (labels)\n",
      "  dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], num_channels))\n",
      "  for label in labels:\n",
      "        #print (type(label))\n",
      "        if label == '0':\n",
      "            label_T.append([1,0])\n",
      "        elif label == '1':\n",
      "            label_T.append([0,1])\n",
      "  newLabels = np.array(label_T)\n",
      "  return dataset,newLabels\n",
      "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
      "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
      "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set (3804, 128, 1) (3804, 2)\n",
        "Validation set (475, 128, 1) (475, 2)\n",
        "Test set (476, 128, 1) (476, 2)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def accuracy(predictions, labels):\n",
      "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
      "          / predictions.shape[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\u6df7\u6dc6\u77e9\u9635\n",
      "\n",
      "def calcuConfusionMatrix(predictions, labels):\n",
      "    truePositiveList = []\n",
      "    trueNegativeList = []\n",
      "    falsePositiveList = []\n",
      "    falseNegativeList = []\n",
      "    print (labels.shape)\n",
      "    for i in range(0,labels.shape[0]):\n",
      "        if np.argmax(predictions, 1)[i] == 1 and np.argmax(labels, 1)[i] == 1:\n",
      "            #true positive\n",
      "            truePositiveList.append(i)\n",
      "        elif np.argmax(predictions, 1)[i] == 0 and np.argmax(labels, 1)[i] == 0:\n",
      "            trueNegativeList.append(i)\n",
      "        elif np.argmax(predictions, 1)[i] == 1 and np.argmax(labels, 1)[i] == 0:\n",
      "            falsePositiveList.append(i)\n",
      "        elif np.argmax(predictions, 1)[i] == 0 and np.argmax(labels, 1)[i] == 1:\n",
      "            falseNegativeList.append(i)\n",
      "        else:\n",
      "            print ('something wrong!')\n",
      "    confusionMatrixRow1= [len(truePositiveList),len(falseNegativeList)]\n",
      "    confusionMatrixRow2 = [len(falsePositiveList),len(trueNegativeLis)]\n",
      "    confusionMatrix = [(confusionMatrixRow1),(confusionMatrixRow2)]\n",
      "    print ('the Confusion Matrix:')\n",
      "    print (np.mat(confusionMatrix))\n",
      "    return  truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList\n",
      "    \n",
      "def calcuF1Score(truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList):\n",
      "        precisionScore = len(truePositiveList)/(0.0 + len(truePositiveList) + len(falsePositiveList))\n",
      "        recallScore = len(truePositiveList)/(0.0 + len(truePositiveList) + len(falseNegativeList))\n",
      "        F1Score = 2*len(truePositiveList)/(0.0 + 2*len(truePositiveList) + len(falsePositiveList) + len(falseNegativeList))\n",
      "        return precisionScore,recallScore,F1Score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "batch_size = 128\n",
      "filter_sizeList = [2,4,8,16,32]\n",
      "embedding_size = lengthEmbedding\n",
      "vocabulary_size = lengthVocabulary\n",
      "depth = 128\n",
      "num_hidden = 32\n",
      "beta = 0.0005\n",
      "keep_drop = 0.6\n",
      "num_steps = 3001\n",
      "sequence_length = sentenceLength\n",
      "num_filters_total = depth * len(filter_sizeList)\n",
      "lr = 0.005\n",
      "\n",
      "graph = tf.Graph()\n",
      "\n",
      "with graph.as_default():\n",
      "\n",
      "  # Input data.\n",
      "  tf_train_dataset = tf.placeholder(\n",
      "    tf.int64, shape=(batch_size,sequence_length,1))\n",
      "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
      "  tf_valid_dataset = tf.constant(valid_dataset)\n",
      "  tf_test_dataset = tf.constant(test_dataset)\n",
      "  #tf_pretrained_embedding = tf.placeholder(tf.float32, shape=(vocabulary_size,embedding_size))\n",
      "  #Variable\n",
      "  embeddings = tf.Variable(wordEmbedding)\n",
      "  #embeddings.assign(tf_pretrained_embedding)\n",
      "\n",
      "  layer1_weightsList = []\n",
      "  layer1_biasesList = []\n",
      "  for i in range(0,len(filter_sizeList)):\n",
      "      layer1_weightsList.append(tf.Variable(tf.truncated_normal([filter_sizeList[i], embedding_size, num_channels, depth], stddev=0.1)))\n",
      "      layer1_biasesList.append(tf.Variable(tf.zeros([depth])))\n",
      "        \n",
      "  #layer2_weights = tf.Variable(tf.truncated_normal([num_filters_total, num_hidden], stddev=0.1))\n",
      "  #layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))      \n",
      "        \n",
      "  layer3_weights = tf.Variable(tf.truncated_normal([num_filters_total, num_labels], stddev=0.1))\n",
      "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
      "        \n",
      "  # Model.\n",
      "  def model(data,dropout_flag,filter_sizeList):\n",
      "            \n",
      "    embed = tf.nn.embedding_lookup(embeddings,data[:,:,0])\n",
      "    \n",
      "    newEmbed = tf.expand_dims(embed , -1)\n",
      "    \n",
      "    allMaxPoolingLayer = []\n",
      "    l2_loss_term = tf.constant(0.0)\n",
      "    i = 0\n",
      "    for filter_size in filter_sizeList:\n",
      "        conv = tf.nn.conv2d( newEmbed, layer1_weightsList[i], [1, 1 , 1, 1], padding='VALID',use_cudnn_on_gpu=True)\n",
      "        hidden = tf.nn.relu(conv + layer1_biasesList[i])\n",
      "        if dropout_flag == 1:\n",
      "            hidden = tf.nn.dropout(hidden,keep_drop)\n",
      "        pooling = tf.nn.max_pool(hidden, [ 1,sequence_length - filter_size + 1,1,1 ] , [1, 1,1, 1], padding='VALID')\n",
      "        allMaxPoolingLayer.append(pooling)\n",
      "        \n",
      "        l2_loss_term = l2_loss_term + tf.nn.l2_loss(layer1_weightsList[i]) + tf.nn.l2_loss(layer1_biasesList[i])\n",
      "        i = i+1\n",
      "    allMaxPoolingLayer_T = tf.concat(3,allMaxPoolingLayer)\n",
      "    \n",
      "    reshape = tf.reshape(allMaxPoolingLayer_T, [ -1, num_filters_total ])\n",
      "    if dropout_flag == 1:\n",
      "        reshape = tf.nn.dropout(reshape,keep_drop)\n",
      "\n",
      "    scores = tf.matmul(reshape, layer3_weights) + layer3_biases\n",
      "    l2_loss_term = l2_loss_term + tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases)\n",
      "\n",
      "    return scores,l2_loss_term\n",
      "  \n",
      "  # Training computation.\n",
      "  #embedding.assign(tf_train_dataset)\n",
      "  logits,l2_loss_term = model(tf_train_dataset,1,filter_sizeList)\n",
      "  loss = tf.reduce_mean(\n",
      "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta* ( l2_loss_term )\n",
      "    \n",
      "  # Optimizer.\n",
      "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
      "  learning_rate = tf.train.exponential_decay(lr, global_step, num_steps, 0.96)\n",
      "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
      "  \n",
      "  # Predictions for the training, validation, and test data.\n",
      "  train_prediction = tf.nn.softmax(logits)\n",
      "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset,0,filter_sizeList)[0])\n",
      "  test_prediction = tf.nn.softmax(model(tf_test_dataset,0,filter_sizeList)[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_plot_x = []\n",
      "train_plot_y = []\n",
      "valid_plot_y = []\n",
      "\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "  tf.initialize_all_variables().run()\n",
      "  print('Initialized')\n",
      "  for step in range(num_steps):\n",
      "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "    batch_data = train_dataset[offset:(offset + batch_size), :, :]\n",
      "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
      "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
      "    _, l, predictions = session.run(\n",
      "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
      "    if (step % 50 == 0):\n",
      "      print('Minibatch loss at step %d: %f' % (step, l))\n",
      "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
      "      train_plot_x.append(step)\n",
      "      train_plot_y.append(accuracy(predictions, batch_labels))\n",
      "        \n",
      "      print('Validation accuracy: %.1f%%' % accuracy(\n",
      "        valid_prediction.eval(), valid_labels))\n",
      "      valid_plot_y.append(accuracy(valid_prediction.eval(), valid_labels))\n",
      "        \n",
      "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
      "  truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList = calcuConfusionMatrix(test_prediction.eval(), test_labels)\n",
      "  print('precision score: %.1f%%    recall score: %.1f%%    F1 score: %.1f%%' % calcuF1Score( truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList ))\n",
      "  plt.plot(train_plot_x, train_plot_y,'b-',label = \"train accuracy\")\n",
      "  plt.plot(train_plot_x, valid_plot_y,'r-',label = \"valid accuracy\")\n",
      "  plt.xlabel(\"step\")\n",
      "  plt.ylabel(\"accuracy\")\n",
      "  plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Minibatch loss at step 0: 21.722780"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 47.7%\n",
        "Validation accuracy: 45.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 50: 11.885869"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 67.2%\n",
        "Validation accuracy: 62.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 100: 35.081810"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 53.9%\n",
        "Validation accuracy: 47.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 150: 8.761900"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 64.1%\n",
        "Validation accuracy: 49.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 200: 8.825879"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 68.8%\n",
        "Validation accuracy: 49.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 250: 9.979494"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 69.5%\n",
        "Validation accuracy: 67.4%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 300: 6.425208"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 68.0%\n",
        "Validation accuracy: 66.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 350: 2.745127"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 76.6%\n",
        "Validation accuracy: 82.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 400: 5.473614"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 63.3%\n",
        "Validation accuracy: 47.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 450: 3.748300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 75.8%\n",
        "Validation accuracy: 74.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 500: 4.770701"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 67.2%\n",
        "Validation accuracy: 66.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 550: 3.987571"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 74.2%\n",
        "Validation accuracy: 67.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 600: 2.269369"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 75.0%\n",
        "Validation accuracy: 82.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 650: 3.271523"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 72.7%\n",
        "Validation accuracy: 68.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 700: 2.094702"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 78.9%\n",
        "Validation accuracy: 82.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 750: 2.204412"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 78.1%\n",
        "Validation accuracy: 81.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 800: 2.212164"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 75.0%\n",
        "Validation accuracy: 77.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 850: 2.105644"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 79.7%\n",
        "Validation accuracy: 80.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 900: 2.155411"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 78.1%\n",
        "Validation accuracy: 78.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 950: 1.585829"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 79.7%\n",
        "Validation accuracy: 77.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1000: 1.342124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 92.2%\n",
        "Validation accuracy: 81.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1050: 1.587085"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 82.0%\n",
        "Validation accuracy: 81.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1100: 1.418231"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 82.8%\n",
        "Validation accuracy: 82.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1150: 1.278593"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 85.9%\n",
        "Validation accuracy: 81.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1200: 1.188434"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 87.5%\n",
        "Validation accuracy: 82.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1250: 1.377121"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 85.2%\n",
        "Validation accuracy: 82.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1300: 1.437938"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 82.8%\n",
        "Validation accuracy: 82.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1350: 1.508668"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 83.6%\n",
        "Validation accuracy: 80.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1400: 1.461282"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 84.4%\n",
        "Validation accuracy: 71.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1450: 1.187969"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 85.9%\n",
        "Validation accuracy: 84.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1500: 1.309471"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 81.2%\n",
        "Validation accuracy: 80.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1550: 1.358162"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 82.8%\n",
        "Validation accuracy: 80.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1600: 1.433688"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 85.9%\n",
        "Validation accuracy: 74.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1650: 1.187597"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 89.1%\n",
        "Validation accuracy: 82.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1700: 1.211317"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 88.3%\n",
        "Validation accuracy: 82.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1750: 1.397627"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 83.6%\n",
        "Validation accuracy: 80.4%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1800: 1.248824"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 89.1%\n",
        "Validation accuracy: 82.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1850: 2.116231"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 70.3%\n",
        "Validation accuracy: 67.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1900: 1.319577"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 84.4%\n",
        "Validation accuracy: 84.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1950: 1.288187"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 84.4%\n",
        "Validation accuracy: 83.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2000: 1.165831"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 89.1%\n",
        "Validation accuracy: 81.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2050: 1.181253"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 88.3%\n",
        "Validation accuracy: 84.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2100: 1.208842"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 88.3%\n",
        "Validation accuracy: 83.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2150: 1.203873"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 87.5%\n",
        "Validation accuracy: 82.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2200: 1.201587"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 89.8%\n",
        "Validation accuracy: 82.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2250: 1.363001"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 88.3%\n",
        "Validation accuracy: 83.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2300: 1.229392"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 90.6%\n",
        "Validation accuracy: 84.4%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2350: 1.059503"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 95.3%\n",
        "Validation accuracy: 83.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2400: 1.475909"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 82.0%\n",
        "Validation accuracy: 82.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2450: 1.166740"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 87.5%\n",
        "Validation accuracy: 83.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2500: 1.292847"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 85.9%\n",
        "Validation accuracy: 79.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2550: 1.434479"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 80.5%\n",
        "Validation accuracy: 78.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2600: 1.060896"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 95.3%\n",
        "Validation accuracy: 80.4%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2650: 1.184520"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 88.3%\n",
        "Validation accuracy: 81.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2700: 1.182912"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 89.1%\n",
        "Validation accuracy: 82.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2750: 1.177731"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 91.4%\n",
        "Validation accuracy: 70.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2800: 1.133497"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 90.6%\n",
        "Validation accuracy: 83.4%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2850: 1.162648"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 89.1%\n",
        "Validation accuracy: 83.4%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2900: 1.059036"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 95.3%\n",
        "Validation accuracy: 82.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 2950: 1.170485"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 86.7%\n",
        "Validation accuracy: 82.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 3000: 1.111578"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 91.4%\n",
        "Validation accuracy: 83.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Test accuracy: 80.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(476, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "4\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "2\n",
        "1\n",
        "2\n",
        "3\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "4\n",
        "2\n",
        "2\n",
        "2\n",
        "3\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "3\n",
        "1\n",
        "1\n",
        "3\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "2\n",
        "2\n",
        "3\n",
        "1\n",
        "1\n",
        "1\n",
        "3\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "3\n",
        "3\n",
        "1\n",
        "4\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "4\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "4\n",
        "1\n",
        "2\n",
        "3\n",
        "1\n",
        "4\n",
        "4\n",
        "1\n",
        "3\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "1\n",
        "2\n",
        "3\n",
        "1\n",
        "2\n",
        "1\n",
        "3\n",
        "1\n",
        "2\n",
        "4\n",
        "2\n",
        "4\n",
        "2\n",
        "2\n",
        "4\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "4\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "3\n",
        "3\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "1\n",
        "2\n",
        "3\n",
        "1\n",
        "3\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "3\n",
        "2\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "3\n",
        "1\n",
        "2\n",
        "1\n",
        "3\n",
        "1\n",
        "3\n",
        "1\n",
        "1\n",
        "2\n",
        "3\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "3\n",
        "2\n",
        "2\n",
        "1\n",
        "3\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "4\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "3\n",
        "1\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "1\n",
        "1\n",
        "3\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "3\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "4\n",
        "1\n",
        "2\n",
        "1\n",
        "4\n",
        "2\n",
        "2\n",
        "3\n",
        "4\n",
        "2\n",
        "2\n",
        "4\n",
        "1\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "3\n",
        "3\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "2\n",
        "1\n",
        "4\n",
        "2\n",
        "3\n",
        "2\n",
        "2\n",
        "2\n",
        "4\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "4\n",
        "1\n",
        "1\n",
        "4\n",
        "1\n",
        "3\n",
        "2\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "1\n",
        "4\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "4\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "3\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "3\n",
        "1\n",
        "1\n",
        "3\n",
        "4\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "3\n",
        "1\n",
        "2\n",
        "3\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "4\n",
        "3\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "3\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "4\n",
        "3\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "4\n",
        "2\n",
        "4\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "1\n",
        "2\n",
        "4\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "4\n",
        "2\n",
        "2\n",
        "1\n",
        "3\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "4\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "2\n",
        "4\n",
        "1\n",
        "2\n",
        "3\n",
        "2\n",
        "4\n",
        "1\n",
        "3\n",
        "1\n",
        "3\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "4\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "2\n",
        "3\n",
        "3\n",
        "3\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "3\n",
        "2\n",
        "2\n",
        "3\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "2\n",
        "2\n",
        "2\n",
        "1\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "2\n",
        "1\n",
        "1\n",
        "3\n",
        "2\n",
        "3\n",
        "1\n",
        "2\n",
        "1\n",
        "2\n",
        "1\n",
        "1\n",
        "1\n",
        "the Confusion Matrix:\n",
        "[[185  34]\n",
        " [185  34]]\n",
        "precision score: 0.8%    recall score: 0.8%    F1 score: 0.8%\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\u4fdd\u5b58truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList\n",
      "####\u5c06\u4e00\u7ef4list\u8f93\u51fa\u5230\u6587\u4ef6####\n",
      "#\u8f93\u5165\uff1a\u6587\u4ef6\u540d\uff08\u7edd\u5bf9\u8def\u5f84\uff09\uff0c\u8f93\u51fa\u7684\u4e8c\u7ef4list\uff08list\uff09\n",
      "#\u8f93\u51fa\uff1a\u65e0\n",
      "def listOutFile(fileName,list):\n",
      "\tf =  open(fileName,'w')\n",
      "\tWriteText = []\n",
      "\tfor everyone in list:\n",
      "\t\tWriteText.append((str(everyone)+' '))\n",
      "\t\tWriteText.append('\\n')\n",
      "\tf.writelines(WriteText)\n",
      "\tf.close()\n",
      "\tprint (\"list output to \"+fileName+\"successfully!\")\n",
      "\n",
      "#filePath = r'media/huxi/\u65b0\u52a0\u53771/NLP_exLab/pythonCode/CNN/data/hotel_annotatedData_utf8/result_analyse//'\n",
      "filePath = './/'\n",
      "resultList = [truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList]\n",
      "for  index,result in enumerate(resultList):\n",
      "    listOutFile(filePath+str(index)+'.txt',result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "list output to .//0.txtsuccessfully!\n",
        "list output to .//1.txtsuccessfully!\n",
        "list output to .//2.txtsuccessfully!\n",
        "list output to .//3.txtsuccessfully!\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}